{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리\n",
    "\n",
    "텍스트 파일을 읽고, 영어와 프랑스어를 분리하여 토큰화한다.\n",
    "\n",
    "이후 전처리 과정을 통해서 인덱스를 부여한다.\n",
    "\n",
    "다음과 같은 특수한 인덱스 값이 있음에 주의한다.\n",
    "\n",
    "`UNK` 0\n",
    "\n",
    "`PAD` 1\n",
    "\n",
    "`SOS` 2\n",
    "\n",
    "`EOS` 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Sung Hyeon Kim's Code (Assisted by Bong Won Jang)\n",
    "# - 2020 06 15 22:28 ☑️\n",
    "#####################################################\n",
    "\n",
    "def tokenize(path_name):\n",
    "\n",
    "    with open(path_name, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "\n",
    "    source_texts = []\n",
    "    target_texts = []\n",
    "    target_labels = []\n",
    "\n",
    "    for line in lines[:100]:\n",
    "        if not line:\n",
    "            break\n",
    "        source_text, target_text = line.split('\\t')\n",
    "        source_text = source_text.strip() \n",
    "        target_text = target_text.strip()\n",
    "                                                                        # -----------Example-----------\n",
    "        encoder_input = source_text.split()                             # ['come', 'on', '!']\n",
    "        decoder_input = (\"<sos> \" + target_text + \" <eos>\").split()     # ['<sos>', 'allez', '!', '<eos>']\n",
    "        target_label = (target_text + \" <eos>\").split()                 # ['allez', '!', '<eos>']\n",
    "\n",
    "        source_texts.append(encoder_input)\n",
    "        target_texts.append(decoder_input)\n",
    "        target_labels.append(target_label)\n",
    "\n",
    "    return source_texts, target_texts, target_labels\n",
    "\n",
    "def preprocess(tokenize_texts):\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "\n",
    "    #################################################\n",
    "    # add unk, pad, sos, eos to dictionary in advance\n",
    "    #################################################\n",
    "\n",
    "    # word2index\n",
    "    word2index['<unk>'] = 0\n",
    "    word2index['<pad>'] = 1\n",
    "    word2index['<sos>'] = 2\n",
    "    word2index['<eos>'] = 3\n",
    "    \n",
    "    #index2word\n",
    "    index2word = {v: k for k, v in word2index.items()}\n",
    "\n",
    "    #################################################\n",
    "    # add other words to dictionary\n",
    "    #################################################\n",
    "    n_word = 4\n",
    "    for text in tokenize_texts:\n",
    "        for word in text:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = n_word\n",
    "                index2word[n_word] = word\n",
    "                n_word += 1\n",
    "\n",
    "    return word2index, index2word\n",
    "\n",
    "def wordtext2indtext(word_texts, word2ind):\n",
    "    ind_texts = []\n",
    "\n",
    "    for word_text in word_texts:\n",
    "        temp_ind_text = []\n",
    "        for word in word_text:\n",
    "            temp_ind_text.append(word2ind[word])\n",
    "\n",
    "        ind_texts.append(temp_ind_text)\n",
    "        \n",
    "    return ind_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델\n",
    "\n",
    "LSTM 셀을 사용한 Attention으로 `Encoder`, `Decoder`를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################################\n",
    "#   Encoder\n",
    "#\n",
    "#   Encoder for seq2seq model with attention mechanism\n",
    "#   This Encoder is based on a LSTM structure\n",
    "############################################\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    ############################################\n",
    "    #   __init__\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - input_size    : the size of input word vocabulary (영어 단어 사전 크기)\n",
    "    #   - hidden_size   : the size of hidden vector and cell vector\n",
    "    ############################################\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size                                                # scalar : We\n",
    "        self.hidden_size = hidden_size                                              # scalar : h\n",
    "        self.cell_size = hidden_size                                                # scalar : h\n",
    "\n",
    "        self.embedding_matrix = nn.Embedding(self.input_size, self.hidden_size)     # matrix : (We * h)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    ############################################\n",
    "    #   forward\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - word_num  : the integer number of a word (영어 단어 번호)\n",
    "    #   - hidden    : hidden vector (h_0 is zero vector)\n",
    "    #   - cell      : cell vector   (c_0 is zero vector)\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - o         : output vector\n",
    "    #   - hn        : next hidden vector   \n",
    "    #   - cn        : next cell vector\n",
    "    ############################################\n",
    "    def forward(self, word_num, hidden, cell):\n",
    "        embedding_vector = self.embedding_matrix.weight[word_num].view(1, 1, -1)            #    matrix : (1 * 1 * h)\n",
    "        o, (hn, cn) = self.lstm(embedding_vector, (hidden, cell))                           #  o matrix : (1 * 1 * h)\n",
    "                                                                                            # hn matrix : (1 * 1 * h)\n",
    "                                                                                            # cn matrix : (1 * 1 * h)\n",
    "        return o, (hn, cn)\n",
    "\n",
    "    ############################################\n",
    "    #   initHidden\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - device     : the integer number of a word\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - initial hidden vector : zero vector\n",
    "    #\n",
    "    #   아직 Pytorch 문법에서 3차원으로 구성해야 하는 이유를 모르겠습니다.\n",
    "    ############################################\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "    ############################################\n",
    "    #   initCell\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - device     : the integer number of a word\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - initial cell vector : zero vector\n",
    "    #\n",
    "    #   아직 Pytorch 문법에서 3차원으로 구성해야 하는 이유를 모르겠습니다.\n",
    "    ############################################\n",
    "    def initCell(self):\n",
    "        return torch.zeros(1, 1, self.cell_size)\n",
    "\n",
    "############################################\n",
    "#   Decoder\n",
    "#\n",
    "#   Decoder for seq2seq model with attention mechanism\n",
    "#   This Decoder is based on a LSTM structure\n",
    "############################################\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    ############################################\n",
    "    #   __init__\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - output_size   : the size of output word vocabulary (프랑스어 단어 사전 크기)\n",
    "    #   - hidden_size   : the size of hidden vector\n",
    "    #   - max_length    : the max length of output sentence\n",
    "    ############################################\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.output_size = output_size                                              # scalar : Wd\n",
    "        self.hidden_size = hidden_size                                              # scalar : h\n",
    "        self.cell_size = hidden_size                                                # scalar : h\n",
    "        \n",
    "        self.embedding_matrix = nn.Embedding(self.output_size, self.hidden_size)    # matrix : (Wd * h)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "        self.out_linear = nn.Linear(self.hidden_size * 2, self.output_size)         # eq : (1 * Wd) = (1 * 2h) x (2h * Wd)\n",
    "\n",
    "    ############################################\n",
    "    #   forward\n",
    "    #   \n",
    "    #   <parameters>                                                       <size>\n",
    "    #   - word_num  : the integer number of a word (프랑스 단어 번호)    :  scalar\n",
    "    #   - hidden    : hidden vector (h_0 is zero vector)                :  h \n",
    "    #   - cell      : cell vector   (c_0 is zero vector)                :  h\n",
    "    #   - hs        : pile of all hidden vector from encoder            :  (N * h)\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - o         : output vector\n",
    "    #   - hn        : next hidden vector   \n",
    "    #   - cn        : next cell vector\n",
    "    ############################################\n",
    "    def forward(self, word_num, hidden, cell, hs):\n",
    "        embedding_vector = self.embedding_matrix(word_num).view(1, 1, -1)       # matrix : (1 * 1 * h)\n",
    "        o, (hn, cn) = self.lstm(embedding_vector, (hidden, cell))               #  o matrix : (1 * 1 * h)\n",
    "                                                                                # hn matrix : (1 * 1 * h)\n",
    "                                                                                # cn matrix : (1 * 1 * h)               \n",
    "\n",
    "        attn_score = torch.mm(hs, hn.view(-1, 1)).view(1, -1)                   # (1 * N) = (N * h) x (h * 1) \n",
    "        attn_distr = F.softmax(attn_score)                                      # (1 * N) = softmax(1 * N)\n",
    "        attn_output = torch.mm(attn_distr, hs)                                  # (1 * h) = (1 * N) x (N * h)\n",
    "\n",
    "        y = F.softmax(self.out_linear(torch.cat((attn_output, hn.view(1, -1)), dim=1))) # (1 * output_size)\n",
    "                                                                                        # = softmax{ (1 * 2h) x (2h * Wd) }\n",
    "        return y, (hn, cn), attn_distr\n",
    "\n",
    "    ############################################\n",
    "    #   initHidden\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - device     : the integer number of a word\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - initial hidden vector : zero vector\n",
    "    #\n",
    "    #   아직 Pytorch 문법에서 3차원으로 구성해야 하는 이유를 모르겠습니다.\n",
    "    ############################################\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "        \n",
    "    ############################################\n",
    "    #   initCell\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - device     : the integer number of a word\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - initial cell vector : zero vector\n",
    "    #\n",
    "    #   아직 Pytorch 문법에서 3차원으로 구성해야 하는 이유를 모르겠습니다.\n",
    "    ############################################\n",
    "    def initCell(self):\n",
    "        return torch.zeros(1, 1, self.cell_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습\n",
    "\n",
    "앞서 구현한 모델을 이용한 학습을 실현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length):\n",
    "    ############################################\n",
    "    #   train (Encoder, Decoder에 맞게 구현해야 함)\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - input_tensor  \n",
    "    #   - target_tensor  \n",
    "    #   - encoder       : Encoder 모듈\n",
    "    #   - decoder       : Decoder 모듈\n",
    "    #   - encoder       : Encoder Optim (SGD)\n",
    "    #   - decoder       : Decoder Optim (SGD)  \n",
    "    #   - criterion     : Loss 계산 (NLLLoss) \n",
    "    #   - max_length    : 문장의 최대 길이\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - encoder       : Encoder 모듈\n",
    "    #   - decoder       : Decoder 모듈  \n",
    "    #   - loss          : loss\n",
    "    ############################################\n",
    "    SOS_token = 2\n",
    "    EOS_token = 3\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_cell = encoder.initCell()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return encoder, decoder, loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_trainer(pairs, max_length, source_size, target_size, hidden_size=256, iteration=1000, learning_rate=0.01):\n",
    "    ############################################\n",
    "    #   LSTM_trainer\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - source_size       : 영어 사전 크기\n",
    "    #   - target_size       : 프랑스어 사전 크기\n",
    "    #   - hidden_size       : 은닉 레이어 크기\n",
    "    #   - pairs             : index로 변환한 영어 및 프랑스어 문장의 쌍 tensor\n",
    "    #   - max_length        : index로 변환한 영어 또는 프랑스어 문장의 최대 길이\n",
    "    #   - iteration         : iteration 횟수 (기본값 1000)\n",
    "    #   - learning_rate     : learning rate (기본값 0.01)  \n",
    "    #\n",
    "    #   <return>\n",
    "    #   - encoder           : Encoder 모듈\n",
    "    #   - decoder           : Decoder 모듈  \n",
    "    ############################################\n",
    "\n",
    "    encoder = Encoder(source_size, hidden_size)\n",
    "    decoder = Decoder(target_size, hidden_size)\n",
    "    start = time.time()\n",
    "    print_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [random.choice(pairs) for i in range(iteration)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        training_pair = training_pairs[i]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        encoder, decoder, loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "            print_loss_avg = print_loss_total / 100\n",
    "            print_loss_total = 0\n",
    "            print(\"{} / {}\\n소요 시간: {}\\n진행률: {}%\\n 평균 손실: {}\".format(iter, iteration, time.time() - start, iter / iteration * 100, print_loss_avg))\n",
    "            \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가\n",
    "\n",
    "학습이 잘 되었는지 `bleu score`을 이용해 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pair, max_length):\n",
    "    ############################################\n",
    "    #   evaluate\n",
    "    #   \n",
    "    #   <parameters>\n",
    "    #   - input_tensor  \n",
    "    #   - target_tensor  \n",
    "    #   - encoder       : Encoder 모듈\n",
    "    #   - decoder       : Decoder 모듈\n",
    "    #   - max_length    : 문장의 최대 길이\n",
    "    #\n",
    "    #   <return>\n",
    "    #   - bleu_score    : bleu 점수\n",
    "    ############################################\n",
    "    input_tensor, target_tensor = pair\n",
    "    \n",
    "    SOS_token = 2\n",
    "    EOS_token = 3\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_cell = encoder.initCell()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "    \n",
    "    return bleu.sentence_bleu([target_tensor.tolist()], [decoder_output.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_evaluate(encoder, decoder, pairs, max_length):\n",
    "    evaluate(encoder, decoder, random.choice(pairs), max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5673740569387403"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures', 'that', 'the', 'military', 'always', 'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures', 'that', 'the', 'military', 'will', 'forever', 'heed', 'Party', 'commands']\n",
    "ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which', 'guarantees', 'the', 'military', 'forces', 'always', 'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n",
    "ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army', 'always', 'to', 'heed', 'the', 'directions', 'of', 'the', 'party']\n",
    "hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was', 'interested', 'in', 'world', 'history']\n",
    "ref2a = ['he', 'was', 'interested', 'in', 'world', 'history', 'because', 'he', 'read', 'the', 'book']\n",
    "\n",
    "bleu.corpus_bleu([[ref1a, ref1b], [ref2a]], [hyp1, hyp2])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "SIZE source_word2index :  45\n",
      "------------------------------------\n",
      "SIZE target_word2index :  109\n",
      "------------------------------------\n",
      "SOURCE WORD2INDEX MAX LENGTH :  4\n",
      "TARGET WORD2INDEX MAX LENGTH :  10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    source_texts, target_texts, target_labels = tokenize('data/eng-fra_train.txt')\n",
    "    \n",
    "    '''\n",
    "    for eng, fra , label_fra in zip(source_texts, target_texts, target_labels):\n",
    "        print(\"(1) ENG :\", eng, \"\\n(2) FRA :\", fra, \"\\n(3) LABEL FRA :\", label_fra, \"\\n\")\n",
    "    '''\n",
    "\n",
    "    source_word2index, source_index2word = preprocess(source_texts)\n",
    "    target_word2index, target_index2word = preprocess(target_texts)\n",
    "    source_size = len(source_word2index)\n",
    "    target_size = len(target_word2index)\n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"SIZE source_word2index : \", source_size)\n",
    "    \n",
    "    '''\n",
    "    print(list(source_word2index.items()))\n",
    "    print(list(source_index2word.items()))\n",
    "    '''\n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"SIZE target_word2index : \", target_size)\n",
    "    \n",
    "    '''\n",
    "    print(list(target_word2index.items()))\n",
    "    print(list(target_index2word.items()))\n",
    "    '''\n",
    "    \n",
    "    source_ind_texts = wordtext2indtext(source_texts, source_word2index)\n",
    "    target_ind_texts = wordtext2indtext(target_texts, target_word2index)\n",
    "    target_ind_labels = wordtext2indtext(target_labels, target_word2index)\n",
    "    \n",
    "    '''\n",
    "    for eng, fra , label_fra in zip(source_ind_texts, target_ind_texts, target_ind_labels):\n",
    "        print(\"(1) ENG :\", eng, \"\\n(2) FRA :\", fra, \"\\n(3) LABEL FRA :\", label_fra, \"\\n\")\n",
    "    '''\n",
    "    \n",
    "    source_max_length = max([len(each) for each in source_ind_texts])\n",
    "    target_max_length = max([len(each) for each in target_ind_texts])\n",
    "    max_length = max(source_max_length, target_max_length)\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"SOURCE WORD2INDEX MAX LENGTH : \", source_max_length)\n",
    "    print(\"TARGET WORD2INDEX MAX LENGTH : \", target_max_length)\n",
    "    \n",
    "    pairs = [(torch.tensor(s, dtype=torch.long).view(-1, 1), \\\n",
    "              torch.tensor(t, dtype=torch.long).view(-1, 1)) \\\n",
    "             for s, t in zip(source_ind_texts, target_ind_labels)]\n",
    "    \n",
    "    # encoder, decoder = LSTM_trainer(pairs, max_length, source_size, target_size)\n",
    "    # random_evaluate(encoder, decoder, pairs, max_length)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
